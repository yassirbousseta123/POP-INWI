import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import pytz
import traceback

class DataCleaner:
    """Syst√®me de nettoyage des donn√©es pour les POPs"""
    
    def __init__(self, data_dir="data"):
        # Convertir en chemin absolu si c'est un chemin relatif
        if not Path(data_dir).is_absolute():
            self.data_dir = Path.cwd() / data_dir
        else:
            self.data_dir = Path(data_dir)
        print(f"üîç Chemin absolu du dossier data : {self.data_dir}")
        
    def get_regions(self):
        """Retourne la liste des r√©gions disponibles"""
        return [d.name for d in self.data_dir.iterdir() if d.is_dir()]
    
    def get_pops(self, region):
        """Retourne la liste des POPs disponibles pour une r√©gion donn√©e"""
        region_path = self.data_dir / region
        if not region_path.exists():
            return []
        return [d.name for d in region_path.iterdir() if d.is_dir()]
        
    def get_pop_path(self, region, pop):
        """Retourne le chemin complet vers un POP sp√©cifique"""
        return self.data_dir / region / pop
        
    def _read_file_header(self, file_path, encoding='ISO-8859-1', num_lines=5):
        """Lit les premi√®res lignes du fichier pour analyse"""
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                lines = [f.readline().strip() for _ in range(num_lines)]
            return lines
        except Exception:
            return []
            
    def _detect_format(self, lines):
        """D√©tecte le format du fichier bas√© sur ses premi√®res lignes"""
        if not lines:
            return None, None, []
        
        header_line = lines[0].replace('√Ø¬ª¬ø', '').replace('\ufeff', '')
        data_line = lines[1] if len(lines) > 1 else ""
        
        # L'en-t√™te utilise g√©n√©ralement des points-virgules
        if ';' in header_line:
            columns = [col.strip() for col in header_line.split(';')]
            # V√©rifier le format des donn√©es
            if data_line:
                # Si la ligne contient plus de virgules que de points-virgules
                comma_count = data_line.count(',')
                semicolon_count = data_line.count(';')
                if comma_count > semicolon_count and comma_count >= 3:
                    return columns, ',', 1
            return columns, ';', 1
        
        # Essayer de d√©tecter le d√©limiteur dans les donn√©es
        delimiters = [',', ';', '|']
        if data_line:
            counts = {d: data_line.count(d) for d in delimiters}
            if counts:
                best_delimiter = max(counts.items(), key=lambda x: x[1])[0]
                return None, best_delimiter, 0
        
        # Par d√©faut, utiliser le point-virgule
        return None, ';', 0

    def load_and_clean_csv(self, file_path, encoding='ISO-8859-1'):
        """Charge et nettoie un fichier CSV"""
        try:
            print(f"\nüìÑ Traitement du fichier : {file_path.name}")
            
            # Essayer diff√©rents encodages
            encodings = ['ISO-8859-1', 'utf-8', 'utf-8-sig']
            lines = []
            used_encoding = None
            
            for enc in encodings:
                try:
                    lines = self._read_file_header(file_path, enc)
                    if lines:
                        used_encoding = enc
                        break
                except UnicodeDecodeError:
                    continue
            
            if not lines:
                print("‚ùå Impossible de lire le fichier avec les encodages standards")
                return pd.DataFrame()
            
            # Afficher les premi√®res lignes pour debug
            print("   ‚ÑπÔ∏è Premi√®res lignes du fichier :")
            for i, line in enumerate(lines):
                print(f"   {i+1}: {line}")
            
            # D√©tecter le format
            columns, delimiter, skiprows = self._detect_format(lines)
            print(f"   üìä D√©limiteur d√©tect√© : '{delimiter}'")
            # Ne pas pr√©d√©finir le nom de colonne, laisser le CSV d√©terminer
            # Les colonnes seront automatiquement lues du fichier
            
            # Lecture du fichier selon son format
            try:
                if columns:
                    df = pd.read_csv(
                        file_path,
                        delimiter=delimiter,
                        names=columns,
                        skiprows=skiprows,
                        encoding=used_encoding,
                        on_bad_lines='skip',
                        quoting=1  # Pour g√©rer les valeurs entre guillemets
                    )
                else:
                    df = pd.read_csv(
                        file_path,
                        delimiter=delimiter,
                        encoding=used_encoding,
                        on_bad_lines='skip',
                        quoting=1
                    )
            except Exception as e:
                print(f"   ‚ùå Premi√®re tentative √©chou√©e : {str(e)}")
                try:
                    # Deuxi√®me tentative sans le mode quoting
                    if columns:
                        df = pd.read_csv(
                            file_path,
                            delimiter=delimiter,
                            names=columns,
                            skiprows=skiprows,
                            encoding=used_encoding,
                            on_bad_lines='skip'
                        )
                    else:
                        df = pd.read_csv(
                            file_path,
                            delimiter=delimiter,
                            encoding=used_encoding,
                            on_bad_lines='skip'
                        )
                except Exception as e2:
                    print(f"   ‚ùå Deuxi√®me tentative √©chou√©e : {str(e2)}")
                    return pd.DataFrame()
            
            # Nettoyer les noms de colonnes
            df.columns = df.columns.str.strip()
            df.columns = [col.replace('\ufeff', '').replace('√Ø¬ª¬ø', '') for col in df.columns]
            print(f"   üìä Colonnes nettoy√©es : {df.columns.tolist()}")
            
            # Traitement plus flexible du Status
            if 'Status' in df.columns:
                df['Status'] = df['Status'].astype(str)
                # Garder les lignes qui ne contiennent pas d'erreur explicite
                error_keywords = ['error', 'failure', 'failed', 'invalid']
                mask = ~df['Status'].str.lower().str.contains('|'.join(error_keywords))
                df = df[mask]
            
            # G√©rer les dates avec support des fuseaux horaires
            if 'Timestamp' in df.columns:
                # Supprimer les suffixes de fuseau horaire et nettoyer
                df['Timestamp'] = (df['Timestamp']
                    .astype(str)
                    .str.replace(r'\s+(WEST|WET|GMT|UTC|CET|CEST)$', '', regex=True)
                    .str.strip())
                # Convertir en datetime avec gestion des erreurs
                df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')
                # Supprimer les lignes avec des dates invalides
                df = df.dropna(subset=['Timestamp'])
            
            # Traiter les valeurs
            for col in df.columns:
                if 'Value' in col:
                    # Convertir en string pour le nettoyage initial
                    df[col] = df[col].astype(str).str.strip()
                    
                    if 'CLIM' in file_path.name and 'Etat' in file_path.name:
                        # Nettoyer et standardiser les valeurs ON/OFF
                        df[col] = df[col].str.upper()
                        on_values = ['ON', 'MARCHE', '1', 'TRUE', 'VRAI']
                        off_values = ['OFF', 'ARRET', 'ARR√äT', '0', 'FALSE', 'FAUX']
                        
                        # Cr√©er le mapping
                        value_map = {v: 1 for v in on_values}
                        value_map.update({v: 0 for v in off_values})
                        df[col] = df[col].map(value_map)
                        
                    elif 'Porte' in file_path.name:
                        df[col] = df[col].str.upper()
                        # Mapping √©tendu pour les √©tats de porte
                        door_map = {
                            'OUVERTE': 1, 'OUVERT': 1, 'OPEN': 1,
                            'FERM√â': 0, 'FERM√âE': 0, 'FERME': 0, 'FERMEE': 0,
                            'FERM√É¬©': 0, 'FERM√É‚Ä∞': 0, 'CLOSED': 0, 'CLOSE': 0,
                            '1': 1, '0': 0, 'TRUE': 1, 'FALSE': 0
                        }
                        df[col] = df[col].map(door_map)
                        
                    elif any(unit in col for unit in ['¬∞C', 'kW']):
                        # Nettoyer les valeurs num√©riques
                        df[col] = (df[col]
                            .str.replace(',', '.')  # Remplacer la virgule par le point d√©cimal
                            .str.replace(r'[^\d.-]+', '', regex=True)  # Garder uniquement les chiffres, le point et le signe
                        )
                        # Convertir en num√©rique et g√©rer les erreurs
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                        # Filtrer les valeurs aberrantes pour la temp√©rature et la puissance
                        if '¬∞C' in col:
                            df.loc[df[col] > 60, col] = np.nan  # Temp√©rature max 60¬∞C
                            df.loc[df[col] < -10, col] = np.nan  # Temp√©rature min -10¬∞C
                        elif 'kW' in col:
                            df.loc[df[col] < 0, col] = 0  # Puissance ne peut pas √™tre n√©gative
                        
                    # V√©rifier si la colonne est enti√®rement vide apr√®s conversion
                    if df[col].isna().all():
                        print(f"‚ö†Ô∏è La colonne {col} est vide apr√®s conversion")
                    else:
                        valid_count = df[col].notna().sum()
                        print(f"‚úì {valid_count} valeurs valides dans {col}")
            
            print(f"‚úÖ Fichier charg√© avec succ√®s ({len(df)} lignes)")
            return df
            
        except Exception as e:
            print(f"‚ùå Erreur g√©n√©rale : {str(e)}")
            return pd.DataFrame()
    
    def load_all_data(self, region=None, pop=None):
        """Charge et nettoie tous les fichiers de donn√©es pour un POP sp√©cifique"""
        # Debug: afficher le chemin complet
        base_path = self.data_dir
        if region and pop:
            base_path = self.data_dir / region / pop
            
        print(f"üìÇ Chemin de base du dossier data : {self.data_dir}")
        print(f"üìÇ Chemin complet du POP : {base_path}")
        
        # V√©rifier si le dossier existe
        if not base_path.exists():
            print(f"‚ùå Le dossier n'existe pas : {base_path}")
            return {}
        
        # Lister tous les fichiers dans le dossier
        print("üìÑ Fichiers trouv√©s dans le dossier :")
        for file in base_path.glob('*.csv'):
            print(f"   - {file.name}")
            
        data_files = {
            'temp_ambiante': 'Temp√©rature Ambiante.csv',
            'temp_exterieure': 'Temp√©rature Ext√©rieure.csv',
            'puissance_clim': 'P.Active CLIM.csv',
            'puissance_generale': 'P.Active G√©n√©rale.csv',
            'clim_a': 'Etat CLIM A.csv',
            'clim_b': 'Etat CLIM B.csv',
            'clim_c': 'Etat CLIM C.csv',
            'clim_d': 'Etat CLIM D.csv',
            'porte': 'Etat Porte.csv'
        }
        
        cleaned_data = {}
        
        # Si r√©gion et POP sont sp√©cifi√©s, utiliser leur chemin
        base_path = self.data_dir
        if region and pop:
            base_path = self.data_dir / region / pop
            
        for key, filename in data_files.items():
            file_path = base_path / filename
            if file_path.exists():
                print(f"üìÇ Chargement de {filename}...")
                df = self.load_and_clean_csv(file_path)
                if not df.empty:
                    cleaned_data[key] = df
                    print(f"‚úÖ {filename} charg√© avec succ√®s ({len(df)} lignes)")
                    # Debug: afficher les colonnes
                    print(f"   Colonnes: {df.columns.tolist()}")
                else:
                    print(f"‚ö†Ô∏è {filename} est vide apr√®s nettoyage")
            else:
                print(f"‚ùå Fichier non trouv√©: {filename}")
                
        return cleaned_data
    
    def calculate_puissance_it(self, cleaned_data):
        """Calcule la Puissance IT = Puissance G√©n√©rale - Puissance CLIM"""
        if 'puissance_generale' in cleaned_data and 'puissance_clim' in cleaned_data:
            df_gen = cleaned_data['puissance_generale']
            df_clim = cleaned_data['puissance_clim']
            
            # Fusionner sur Timestamp
            merged = pd.merge(
                df_gen[['Timestamp', 'Value (kW)']],
                df_clim[['Timestamp', 'Value (kW)']],
                on='Timestamp',
                how='inner',
                suffixes=('_generale', '_clim')
            )
            
            # Calculer Puissance IT
            merged['Puissance_IT (kW)'] = merged['Value (kW)_generale'] - merged['Value (kW)_clim']
            
            return merged[['Timestamp', 'Puissance_IT (kW)']]
        
        return pd.DataFrame()
    
    def validate_data(self, df, expected_columns=None):
        """Valide la qualit√© des donn√©es charg√©es"""
        if df.empty:
            print("‚ö†Ô∏è DataFrame vide!")
            return False

        print("\nüîç Validation des donn√©es:")
        
        # V√©rifier les colonnes requises
        if expected_columns:
            missing_cols = set(expected_columns) - set(df.columns)
            if missing_cols:
                print(f"‚ö†Ô∏è Colonnes manquantes: {missing_cols}")
                return False
        
        # V√©rifier la couverture temporelle
        if 'Timestamp' in df.columns:
            time_range = df['Timestamp'].max() - df['Timestamp'].min()
            expected_rows = time_range.total_seconds() / 900  # Pour 15 minutes d'intervalle
            coverage = len(df) / expected_rows if expected_rows > 0 else 0
            print(f"üìä Couverture temporelle: {coverage:.1%}")
            if coverage < 0.5:  # Moins de 50% de couverture
                print("‚ö†Ô∏è Couverture temporelle insuffisante!")
                return False
        
        # V√©rifier les valeurs manquantes par colonne
        for col in df.columns:
            if col == 'Timestamp':
                continue
            null_pct = df[col].isna().mean() * 100
            if null_pct > 50:  # Plus de 50% de valeurs manquantes
                print(f"‚ö†Ô∏è {col}: {null_pct:.1f}% valeurs manquantes")
                return False
            elif null_pct > 0:
                print(f"‚ÑπÔ∏è {col}: {null_pct:.1f}% valeurs manquantes")
        
        print("‚úÖ Validation r√©ussie")
        return True
        
    def merge_all_data(self, cleaned_data):
        """Fusionne toutes les donn√©es sur Timestamp"""
        if not cleaned_data:
            return pd.DataFrame()
        
        dfs_to_merge = []
        
        # Fonction helper pour trouver la colonne de valeur
        def find_value_column(df):
            value_cols = [col for col in df.columns 
                         if any(x in col.lower() for x in ['value', 'valeur']) and 
                            any(x in col for x in ['¬∞C', 'kW', '√Ç¬∞C'])]
            if not value_cols:
                # Fallback to any column with 'Value' in the name
                value_cols = [col for col in df.columns if 'Value' in col]
            return value_cols[0] if value_cols else None
        
        # Temp√©rature ambiante
        if 'temp_ambiante' in cleaned_data:
            df = cleaned_data['temp_ambiante'].copy()
            value_col = find_value_column(df)
            if value_col and 'Timestamp' in df.columns:
                df = df[['Timestamp', value_col]]
                df.rename(columns={value_col: 'Temp_Ambiante'}, inplace=True)
                dfs_to_merge.append(df)
        
        # Temp√©rature ext√©rieure
        if 'temp_exterieure' in cleaned_data:
            df = cleaned_data['temp_exterieure'].copy()
            value_col = find_value_column(df)
            if value_col and 'Timestamp' in df.columns:
                df = df[['Timestamp', value_col]]
                df.rename(columns={value_col: 'Temp_Exterieure'}, inplace=True)
                dfs_to_merge.append(df)
        
        # Traitement des puissances et calcul de la Puissance IT
        if 'puissance_generale' in cleaned_data and 'puissance_clim' in cleaned_data:
            # Pr√©parer les DataFrames de puissance
            df_gen = cleaned_data['puissance_generale'].copy()
            df_clim = cleaned_data['puissance_clim'].copy()
            
            value_col_gen = find_value_column(df_gen)
            value_col_clim = find_value_column(df_clim)
            
            if value_col_gen and value_col_clim and 'Timestamp' in df_gen.columns:
                # Pr√©parer DataFrame de puissance g√©n√©rale
                df_gen = df_gen[['Timestamp', value_col_gen]]
                df_gen.rename(columns={value_col_gen: 'Puissance_Generale'}, inplace=True)
                
                # Pr√©parer DataFrame de puissance CLIM
                df_clim = df_clim[['Timestamp', value_col_clim]]
                df_clim.rename(columns={value_col_clim: 'Puissance_CLIM'}, inplace=True)
                
                # Fusionner les donn√©es de puissance
                puissance_df = pd.merge(df_gen, df_clim, on='Timestamp', how='outer')
                
                # Convertir en num√©rique et g√©rer les valeurs manquantes
                puissance_df['Puissance_Generale'] = pd.to_numeric(puissance_df['Puissance_Generale'], errors='coerce')
                puissance_df['Puissance_CLIM'] = pd.to_numeric(puissance_df['Puissance_CLIM'], errors='coerce')
                
                # Calculer la Puissance IT
                puissance_df['Puissance_IT'] = puissance_df['Puissance_Generale'] - puissance_df['Puissance_CLIM']
                
                # Nettoyer les valeurs aberrantes
                puissance_df.loc[puissance_df['Puissance_IT'] < 0, 'Puissance_IT'] = 0
                
                # Ajouter les colonnes de puissance au DataFrame √† fusionner (une seule fois)
                dfs_to_merge.append(puissance_df[['Timestamp', 'Puissance_Generale', 'Puissance_CLIM', 'Puissance_IT']])
                
                # Afficher les statistiques
                valid_data = puissance_df['Puissance_IT'].notna().sum()
                if valid_data > 0:
                    print(f"\n‚úÖ Puissance IT calcul√©e avec succ√®s:")
                    print(f"   - {valid_data} points de donn√©es valides")
                    print(f"   - Min: {puissance_df['Puissance_IT'].min():.2f} kW")
                    print(f"   - Max: {puissance_df['Puissance_IT'].max():.2f} kW")
                    print(f"   - Moyenne: {puissance_df['Puissance_IT'].mean():.2f} kW")
        elif 'puissance_generale' in cleaned_data:
            # Si on a seulement puissance g√©n√©rale (sans CLIM)
            df = cleaned_data['puissance_generale'].copy()
            value_col = find_value_column(df)
            if value_col and 'Timestamp' in df.columns:
                df = df[['Timestamp', value_col]]
                df.rename(columns={value_col: 'Puissance_Generale'}, inplace=True)
                dfs_to_merge.append(df)
        elif 'puissance_clim' in cleaned_data:
            # Si on a seulement puissance CLIM (sans g√©n√©rale)
            df = cleaned_data['puissance_clim'].copy()
            value_col = find_value_column(df)
            if value_col and 'Timestamp' in df.columns:
                df = df[['Timestamp', value_col]]
                df.rename(columns={value_col: 'Puissance_CLIM'}, inplace=True)
                dfs_to_merge.append(df)
        
        # √âtats CLIM
        for clim in ['clim_a', 'clim_b', 'clim_c', 'clim_d']:
            if clim in cleaned_data:
                df = cleaned_data[clim].copy()
                value_col = find_value_column(df)
                if value_col and 'Timestamp' in df.columns:
                    df = df[['Timestamp', value_col]]
                    df.rename(columns={value_col: f"CLIM_{clim[-1].upper()}_Status"}, inplace=True)
                    dfs_to_merge.append(df)
        
        # √âtat Porte
        if 'porte' in cleaned_data:
            df = cleaned_data['porte'].copy()
            value_col = find_value_column(df)
            if value_col and 'Timestamp' in df.columns:
                df = df[['Timestamp', value_col]]
                df.rename(columns={value_col: 'Porte_Status'}, inplace=True)
                dfs_to_merge.append(df)
        
        # Fusionner tous les DataFrames
        if dfs_to_merge:
            try:
                print("\nüîÑ Fusion des donn√©es...")
                # Afficher un aper√ßu des DataFrames avant la fusion
                for i, df in enumerate(dfs_to_merge):
                    print(f"\nDataFrame {i+1}:")
                    print(f"Colonnes: {df.columns.tolist()}")
                    print(f"P√©riode: {df['Timestamp'].min()} √† {df['Timestamp'].max()}")
                    print(f"Nombre de lignes: {len(df)}")
                
                # Fusion progressive avec v√©rification
                merged = dfs_to_merge[0]
                for i, df in enumerate(dfs_to_merge[1:], 1):
                    before_merge = len(merged)
                    merged = pd.merge(merged, df, on='Timestamp', how='outer')
                    after_merge = len(merged)
                    
                    print(f"\nFusion {i}: {before_merge} ‚Üí {after_merge} lignes")
                    if after_merge == 0:
                        print("‚ö†Ô∏è La fusion a r√©sult√© en 0 lignes!")
                        # V√©rifier les plages de dates
                        print(f"Plage de dates du DataFrame principal: {merged['Timestamp'].min()} √† {merged['Timestamp'].max()}")
                        print(f"Plage de dates du DataFrame √† fusionner: {df['Timestamp'].min()} √† {df['Timestamp'].max()}")
                
                # Trier par Timestamp
                merged = merged.sort_values('Timestamp').reset_index(drop=True)
                
                # Forward-fill pour les donn√©es continues
                continuous_cols = ['Temp_Ambiante', 'Temp_Exterieure', 
                                 'Puissance_CLIM', 'Puissance_Generale', 'Puissance_IT']
                for col in continuous_cols:
                    if col in merged.columns:
                        nulls_before = merged[col].isna().sum()
                        merged[col] = merged[col].ffill(limit=30)
                        nulls_after = merged[col].isna().sum()
                        if nulls_before - nulls_after > 0:
                            print(f"\n{col}: {nulls_before - nulls_after} valeurs combl√©es")
                
                # V√©rification finale
                print("\n‚úÖ Fusion termin√©e")
                print(f"Nombre total de lignes: {len(merged)}")
                print(f"P√©riode couverte: {merged['Timestamp'].min()} √† {merged['Timestamp'].max()}")
                print("Colonnes disponibles:")
                for col in merged.columns:
                    non_null = merged[col].notna().sum()
                    print(f"- {col}: {non_null} valeurs non-null ({non_null/len(merged)*100:.1f}%)")
                
                return merged
                
            except Exception as e:
                print(f"\n‚ùå Erreur lors de la fusion: {str(e)}")
                print("D√©tails de l'erreur:")
                import traceback
                print(traceback.format_exc())
                return pd.DataFrame()
        
        return pd.DataFrame()
    
    def load_multiple_pops(self, pop_list=None, regions=None):
        """
        Charge les donn√©es pour plusieurs POPs √† la fois
        
        Args:
            pop_list: Liste de tuples (region, pop) √† charger
            regions: Liste de r√©gions - charge tous les POPs de ces r√©gions
        
        Returns:
            Dict avec cl√© (region, pop) et valeur les donn√©es fusionn√©es
        """
        all_pops_data = {}
        
        # Si des r√©gions sont sp√©cifi√©es, obtenir tous les POPs de ces r√©gions
        if regions:
            pop_list = []
            for region in regions:
                pops = self.get_pops(region)
                for pop in pops:
                    pop_list.append((region, pop))
        
        # Si aucune liste n'est fournie, charger TOUS les POPs
        if not pop_list:
            pop_list = []
            for region in self.get_regions():
                pops = self.get_pops(region)
                for pop in pops:
                    pop_list.append((region, pop))
        
        print(f"\nüîÑ Chargement de {len(pop_list)} POPs...")
        
        for region, pop in pop_list:
            print(f"\nüìç Traitement de {region}/{pop}...")
            try:
                # Charger les donn√©es du POP
                cleaned_data = self.load_all_data(region=region, pop=pop)
                
                if cleaned_data:
                    # Fusionner les donn√©es
                    merged_data = self.merge_all_data(cleaned_data)
                    
                    if not merged_data.empty:
                        # Ajouter les m√©tadonn√©es du POP
                        merged_data['Region'] = region
                        merged_data['POP'] = pop
                        merged_data['POP_ID'] = f"{region}_{pop}"
                        
                        all_pops_data[(region, pop)] = merged_data
                        print(f"‚úÖ {region}/{pop}: {len(merged_data)} lignes charg√©es")
                    else:
                        print(f"‚ö†Ô∏è {region}/{pop}: Donn√©es vides apr√®s fusion")
                else:
                    print(f"‚ö†Ô∏è {region}/{pop}: Aucune donn√©e trouv√©e")
                    
            except Exception as e:
                print(f"‚ùå Erreur lors du chargement de {region}/{pop}: {str(e)}")
                continue
        
        print(f"\n‚úÖ {len(all_pops_data)} POPs charg√©s avec succ√®s sur {len(pop_list)}")
        return all_pops_data
    
    def calculate_pop_correlations(self, all_pops_data, metric='Temp_Ambiante', period=None):
        """
        Calcule les corr√©lations entre un m√©trique donn√© et d'autres m√©triques pour tous les POPs
        
        Args:
            all_pops_data: Dict avec les donn√©es de tous les POPs
            metric: M√©trique principal pour la corr√©lation (d√©faut: Temp_Ambiante)
            period: Tuple (start_date, end_date) pour filtrer la p√©riode
        
        Returns:
            DataFrame avec les corr√©lations pour chaque POP
        """
        correlation_results = []
        
        # Liste des m√©triques √† corr√©ler avec la m√©trique principale
        correlation_metrics = ['Temp_Exterieure', 'Puissance_IT', 'Puissance_CLIM', 
                              'CLIM_A_Status', 'CLIM_B_Status', 'CLIM_C_Status', 
                              'CLIM_D_Status', 'Porte_Status']
        
        for (region, pop), data in all_pops_data.items():
            # Filtrer par p√©riode si sp√©cifi√©e
            if period and 'Timestamp' in data.columns:
                start_date, end_date = period
                mask = (data['Timestamp'] >= start_date) & (data['Timestamp'] <= end_date)
                data = data[mask]
            
            if len(data) < 10:  # Besoin d'au moins 10 points
                continue
            
            # V√©rifier que la m√©trique principale existe
            if metric not in data.columns:
                continue
            
            # Calculer les corr√©lations
            pop_correlations = {
                'Region': region,
                'POP': pop,
                'POP_ID': f"{region}_{pop}",
                'Data_Points': len(data),
                'Period_Start': data['Timestamp'].min() if 'Timestamp' in data.columns else None,
                'Period_End': data['Timestamp'].max() if 'Timestamp' in data.columns else None
            }
            
            # Calculer la corr√©lation avec chaque m√©trique
            for corr_metric in correlation_metrics:
                if corr_metric in data.columns:
                    # Pr√©parer les donn√©es pour la corr√©lation
                    corr_data = data[[metric, corr_metric]].dropna()
                    
                    if len(corr_data) >= 10:
                        # Convertir les colonnes de statut en num√©rique si n√©cessaire
                        if corr_metric == 'Porte_Status' and corr_data[corr_metric].dtype == 'object':
                            corr_data[corr_metric] = corr_data[corr_metric].map({
                                'Open': 1, 'Ouvert': 1, 'open': 1, '1': 1, 1: 1,
                                'Close': 0, 'Ferm√©': 0, 'closed': 0, '0': 0, 0: 0
                            })
                        
                        if 'CLIM' in corr_metric and 'Status' in corr_metric:
                            if corr_data[corr_metric].dtype == 'object':
                                corr_data[corr_metric] = corr_data[corr_metric].map({
                                    'ON': 1, 'on': 1, 'On': 1, '1': 1, 1: 1,
                                    'OFF': 0, 'off': 0, 'Off': 0, '0': 0, 0: 0
                                })
                        
                        # Calculer les corr√©lations
                        try:
                            pearson_corr = corr_data[metric].corr(corr_data[corr_metric], method='pearson')
                            spearman_corr = corr_data[metric].corr(corr_data[corr_metric], method='spearman')
                            
                            pop_correlations[f'{corr_metric}_Pearson'] = pearson_corr
                            pop_correlations[f'{corr_metric}_Spearman'] = spearman_corr
                            pop_correlations[f'{corr_metric}_Count'] = len(corr_data)
                        except:
                            pop_correlations[f'{corr_metric}_Pearson'] = None
                            pop_correlations[f'{corr_metric}_Spearman'] = None
                            pop_correlations[f'{corr_metric}_Count'] = 0
                    else:
                        pop_correlations[f'{corr_metric}_Pearson'] = None
                        pop_correlations[f'{corr_metric}_Spearman'] = None
                        pop_correlations[f'{corr_metric}_Count'] = 0
                else:
                    pop_correlations[f'{corr_metric}_Pearson'] = None
                    pop_correlations[f'{corr_metric}_Spearman'] = None
                    pop_correlations[f'{corr_metric}_Count'] = 0
            
            # Calculer aussi les statistiques de base pour la m√©trique principale
            pop_correlations[f'{metric}_Mean'] = data[metric].mean()
            pop_correlations[f'{metric}_Std'] = data[metric].std()
            pop_correlations[f'{metric}_Min'] = data[metric].min()
            pop_correlations[f'{metric}_Max'] = data[metric].max()
            
            correlation_results.append(pop_correlations)
        
        # Cr√©er un DataFrame avec tous les r√©sultats
        if correlation_results:
            return pd.DataFrame(correlation_results)
        else:
            return pd.DataFrame()


if __name__ == "__main__":
    # Test du syst√®me de nettoyage
    cleaner = DataCleaner()
    data = cleaner.load_all_data()
    print(f"\nüìä Nombre total de fichiers charg√©s: {len(data)}")
    
    # Test de la fusion
    merged_data = cleaner.merge_all_data(data)
    if not merged_data.empty:
        print(f"\n‚úÖ Donn√©es fusionn√©es: {merged_data.shape}")
        print("\nColonnes disponibles:")
        for col in merged_data.columns:
            print(f"  - {col}")